{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pranesh1306/Pranesh1306/blob/main/I_UNET.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Dxg6oVWlveUl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "e3_Lqps1vgEu"
      },
      "outputs": [],
      "source": [
        "def preprocess_image(image_path, target_size):\n",
        "    image = cv2.imread(image_path)\n",
        "    image = cv2.resize(image, target_size, interpolation=cv2.INTER_NEAREST)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert image to RGB\n",
        "    image = image / 255.0  # Normalize by dividing by 255\n",
        "    return image\n",
        "\n",
        "def preprocess_label(label_path, target_size):\n",
        "    label = cv2.imread(label_path)\n",
        "    label = cv2.resize(label, target_size, interpolation=cv2.INTER_NEAREST)\n",
        "    label = cv2.cvtColor(label, cv2.COLOR_BGR2RGB)  # Convert image to RGB\n",
        "    label = label / 255.0  # Normalize by dividing by 255\n",
        "    return label\n",
        "\n",
        "def preprocess_data(image_folder, label_folder, target_size):\n",
        "    images = []\n",
        "    labels = []\n",
        "\n",
        "    image_files = os.listdir(image_folder)\n",
        "    for image_file in image_files:\n",
        "        image_path = os.path.join(image_folder, image_file)\n",
        "        label_path = os.path.join(label_folder, image_file)\n",
        "\n",
        "        image = preprocess_image(image_path, target_size)\n",
        "        label = preprocess_image(label_path, target_size)\n",
        "\n",
        "        images.append(image)\n",
        "        labels.append(label)\n",
        "\n",
        "    images = np.array(images)\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    return images, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8M-16uaKvocV"
      },
      "outputs": [],
      "source": [
        "# Paths to the image and label folders\n",
        "import zipfile\n",
        "zip_file_path_1 = \"images.zip\"\n",
        "zip_file_path_2 = \"labels.zip\"\n",
        "\n",
        "# Extract the contents of the zip file\n",
        "with zipfile.ZipFile(zip_file_path_1, 'r') as zip_ref:\n",
        "    zip_ref.extractall('images')  # Extract to the 'images' directory\n",
        "# Extract the contents of the zip file\n",
        "with zipfile.ZipFile(zip_file_path_2, 'r') as zip_ref:\n",
        "    zip_ref.extractall('labels')  # Extract to the 'images' directory    \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1BGEyhLHz7yT"
      },
      "outputs": [],
      "source": [
        "image_folder = \"/content/images\"\n",
        "label_folder = \"/content/labels\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "abrPDYAAvrq4"
      },
      "outputs": [],
      "source": [
        "# Preprocess the data\n",
        "target_size = (256, 256)\n",
        "images, labels = preprocess_data(image_folder,label_folder, target_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJweRUrh0Pkh",
        "outputId": "cce5ecfe-c1a0-4784-b7cd-6413a9ca98f0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(220, 256, 256, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "np.shape(images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "WQsxm5TE0UIW"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Create an instance of the ImageDataGenerator\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=90,\n",
        "    width_shift_range=0.3,\n",
        "    height_shift_range=0.3,\n",
        "    shear_range=0.3,\n",
        "    zoom_range=0.3\n",
        ")\n",
        "# Convert the tuples to NumPy arrays\n",
        "images = np.array(images)\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Reshape the images and labels to match the expected input shape\n",
        "reshaped_images = images.reshape(-1, target_size[0], target_size[1], 3)\n",
        "reshaped_labels = labels.reshape(-1, target_size[0], target_size[1], 3)\n",
        "\n",
        "# Generate augmented images\n",
        "augmented_images = []\n",
        "augmented_labels = []\n",
        "for image in reshaped_images:\n",
        "    augmented_images.extend(datagen.flow(np.expand_dims(image, axis=0), batch_size=1)[0])\n",
        "for label in reshaped_labels:\n",
        "    augmented_labels.extend(datagen.flow(np.expand_dims(label, axis=0), batch_size=1)[0])\n",
        "\n",
        "# Convert the augmented images back to numpy array format\n",
        "augmented_images = np.array(augmented_images)\n",
        "augmented_labels = np.array(augmented_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_53y_Ge0ZeZ"
      },
      "outputs": [],
      "source": [
        "# Combine original and augmented images for training\n",
        "combined_images = np.concatenate((images, augmented_images), axis=0)\n",
        "combined_labels = np.concatenate((labels, augmented_labels), axis=0)\n",
        "\n",
        "# Convert the combined dataset back to separate arrays\n",
        "combined_images = np.array(list(combined_images))\n",
        "combined_labels = np.array(list(combined_labels))\n",
        "\n",
        "train_images, test_images, train_labels, test_labels = train_test_split(combined_images,combined_labels, test_size=0.2, random_state=42)\n",
        "train_images = np.array(train_images)\n",
        "test_images = np.array(test_images)\n",
        "train_labels = np.array(train_labels)\n",
        "test_labels = np.array(test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUHxgZuQGbCm"
      },
      "outputs": [],
      "source": [
        "np.shape(train_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cEg9FZlaCka7"
      },
      "outputs": [],
      "source": [
        "!pip install efficientnet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, Conv2DTranspose, Concatenate, Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
        "from efficientnet.tfkeras import EfficientNetB0\n",
        "\n",
        "def unet_model(input_shape):\n",
        "    inputs = Input(input_shape)\n",
        "\n",
        "    # Downsample path\n",
        "    conv1 = Conv2D(64, 3, activation='relu', padding='same')(inputs)\n",
        "    conv1 = Conv2D(64, 3, activation='relu', padding='same')(conv1)\n",
        "    pool1 = MaxPool2D(pool_size=(2, 2))(conv1)\n",
        "\n",
        "    conv2 = Conv2D(128, 3, activation='relu', padding='same')(pool1)\n",
        "    conv2 = Conv2D(128, 3, activation='relu', padding='same')(conv2)\n",
        "    pool2 = MaxPool2D(pool_size=(2, 2))(conv2)\n",
        "\n",
        "    # Upsample path\n",
        "    up4 = tf.keras.layers.UpSampling2D(size=(2, 2))(pool2)\n",
        "    up4 = Conv2D(128, 2, activation='relu', padding='same')(up4)\n",
        "    merge4 = Concatenate()([conv2, up4])\n",
        "    conv4 = Conv2D(128, 3, activation='relu', padding='same')(merge4)\n",
        "    conv4 = Conv2D(128, 3, activation='relu', padding='same')(conv4)\n",
        "\n",
        "    up5 = tf.keras.layers.UpSampling2D(size=(2, 2))(conv4)\n",
        "    up5 = Conv2D(64, 2, activation='relu', padding='same')(up5)\n",
        "    merge5 = Concatenate()([conv1, up5])\n",
        "    conv5 = Conv2D(64, 3, activation='relu', padding='same')(merge5)\n",
        "    conv5 = Conv2D(64, 3, activation='relu', padding='same')(conv5)\n",
        "\n",
        "    # Output\n",
        "    outputs = Conv2D(3, 1, activation='sigmoid')(conv5)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "# Create the U-Net model with EfficientNet backbone\n",
        "input_shape = (256, 256, 3)  # Adjust if necessary\n",
        "model = unet_model(input_shape)\n",
        "\n",
        "# Load pre-trained EfficientNet weights\n",
        "base_model = EfficientNetB0(input_shape=input_shape, include_top=False, weights='imagenet')\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Define the early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "# Define the learning rate schedule\n",
        "def lr_schedule(epoch):\n",
        "    if epoch < 5:\n",
        "        return 0.001  # High initial learning rate for fast convergence\n",
        "    else:\n",
        "        return 0.0001  # Lower learning rate for fine-tuning\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "batch_size = 16  # Adjust if necessary\n",
        "epochs = 10 # Adjust if necessary\n",
        "model.fit(\n",
        "    train_images, train_labels,\n",
        "    validation_data=(test_images, test_labels),\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    callbacks=[early_stopping, lr_scheduler]\n",
        ")\n"
      ],
      "metadata": {
        "id": "YMZ6hsciYuD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions = model.predict(test_images)\n",
        "\n",
        "# Calculate metrics\n",
        "def calculate_iou(y_true, y_pred):\n",
        "    intersection = np.logical_and(y_true, y_pred)\n",
        "    union = np.logical_or(y_true, y_pred)\n",
        "    iou = np.sum(intersection) / np.sum(union)\n",
        "    return iou\n",
        "\n",
        "def calculate_specificity(y_true, y_pred):\n",
        "    true_negative = np.sum((1 - y_true) * (1 - y_pred))\n",
        "    true_negative_rate = true_negative / np.sum(1 - y_true)\n",
        "    return true_negative_rate\n",
        "\n",
        "def calculate_sensitivity(y_true, y_pred):\n",
        "    true_positive = np.sum(y_true * y_pred)\n",
        "    true_positive_rate = true_positive / np.sum(y_true)\n",
        "    return true_positive_rate\n",
        "\n",
        "accuracy = model.evaluate(test_images, test_labels)[1]\n",
        "iou = calculate_iou(test_labels, predictions)\n",
        "specificity = calculate_specificity(test_labels, predictions)\n",
        "sensitivity = calculate_sensitivity(test_labels, predictions)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"IOU:\", iou)\n",
        "print(\"Specificity:\", specificity)\n",
        "print(\"Sensitivity:\", sensitivity)"
      ],
      "metadata": {
        "id": "s7ywpz4fSNjl"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}